{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a029c2aa",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kiankyars/Ultra-Scale-Playbook-Series/blob/main/2_activation_memory_recomputation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a18149",
   "metadata": {},
   "source": [
    "# Ultra-Scale Playbook: Part 2\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "\n",
    "- Memory costs per parameter in FP32 and mixed precision (BF16)\n",
    "- What activation memory is and why it becomes the dominant cost at scale\n",
    "- Gradient checkpointing (activation recomputation) to reduce memory usage\n",
    "- Measuring memory and recomputation tradeoffs in a toy transformer\n",
    "\n",
    "This notebook accompanies Video 2 in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory footprint breakdown\n",
    "fp32_bytes = 4  # 4 bytes per parameter\n",
    "momentum_bytes = 4\n",
    "variance_bytes = 4\n",
    "total_optimizer_bytes = fp32_bytes + momentum_bytes + variance_bytes\n",
    "\n",
    "print(f\"Total per-parameter memory (FP32): {total_optimizer_bytes} bytes\")\n",
    "\n",
    "# Mixed precision (BF16)\n",
    "bf16_param = 2\n",
    "bf16_grad = 2\n",
    "master_weight = 4\n",
    "\n",
    "total_mp_bytes = bf16_param + bf16_grad + master_weight + momentum_bytes + variance_bytes\n",
    "print(f\"Total per-parameter memory (Mixed Precision): {total_mp_bytes} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c2a1a",
   "metadata": {},
   "source": [
    "### What are activations?\n",
    "\n",
    "Activations are the intermediate outputs during the forward pass of a model. These are cached to compute gradients during the backward pass (via chain rule).\n",
    "\n",
    "Their memory grows **linearly** with:\n",
    "\n",
    "- Batch size\n",
    "- Sequence length\n",
    "- Hidden dimension\n",
    "- Number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24047377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulate activation memory growth\n",
    "context_lengths = [512, 1024, 2048, 4096, 8192]\n",
    "layers = 24\n",
    "hidden_dim = 4096\n",
    "batch_size = 1\n",
    "\n",
    "# Each activation token size: 2 bytes * 2 * hidden_dim (Q/K/V)\n",
    "bytes_per_token = 2 * hidden_dim * 2  \n",
    "\n",
    "activation_memory = [cl * batch_size * bytes_per_token * layers / 1e9 for cl in context_lengths]\n",
    "\n",
    "plt.plot(context_lengths, activation_memory, marker='o')\n",
    "plt.title(\"Activation Memory vs. Context Length\")\n",
    "plt.xlabel(\"Context Length (tokens)\")\n",
    "plt.ylabel(\"Activation Memory (GB)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24241167",
   "metadata": {},
   "source": [
    "## Gradient Checkpointing Demo\n",
    "\n",
    "We trade compute for memory by recomputing activations during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class ToyBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(4096, 4096)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4096, 4096)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "block = ToyBlock()\n",
    "x = torch.randn(8, 4096, requires_grad=True)\n",
    "\n",
    "# Regular forward\n",
    "out_regular = block(x)\n",
    "loss_regular = out_regular.mean()\n",
    "loss_regular.backward()\n",
    "print(\"Done regular forward/backward\")\n",
    "\n",
    "# Clear gradients\n",
    "block.zero_grad()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# With gradient checkpointing\n",
    "x_cp = x.detach().requires_grad_()\n",
    "out_cp = checkpoint(block, x_cp)\n",
    "loss_cp = out_cp.mean()\n",
    "loss_cp.backward()\n",
    "print(\"Done checkpointing forward/backward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25521cbe",
   "metadata": {},
   "source": [
    "### Quick Quiz\n",
    "\n",
    "1. Why does activation memory grow linearly with sequence length?\n",
    "2. What trade-off does gradient checkpointing make?\n",
    "3. In what case is full recomputation useful?\n",
    "4. Which component benefits most from recomputation in Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62075512",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "1. Each token generates intermediate outputs that must be stored.\n",
    "2. Reduces memory usage by adding extra computation.\n",
    "3. When GPU memory is very limited but training time is less critical.\n",
    "4. Attention layers (large activations, cheap recompute)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
