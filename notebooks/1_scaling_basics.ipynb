{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9edd378c",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiankyars/Ultra-Scale-Playbook-Series/blob/main/1_scaling_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b986a5b8",
      "metadata": {
        "id": "b986a5b8"
      },
      "source": [
        "# Video 1: What is Scaling in LLM Training?\n",
        "Welcome to the first notebook of the Ultra-Scale Playbook series!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e631d1c6",
      "metadata": {
        "id": "e631d1c6"
      },
      "source": [
        "Objective\n",
        "- Estimate memory usage of a Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfc6b6bf",
      "metadata": {
        "id": "bfc6b6bf"
      },
      "source": [
        "Exercise 1: Estimate Transformer Memory Usage (4 bytes per param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7816b2c",
      "metadata": {
        "id": "f7816b2c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a basic Transformer block\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, ff_dim=3072):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        ff = self.linear2(torch.relu(self.linear1(x)))\n",
        "        return self.norm2(x + ff)\n",
        "\n",
        "model = MiniTransformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qrFEpZ2nYxKN",
      "metadata": {
        "id": "qrFEpZ2nYxKN"
      },
      "source": [
        "Show calculation below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JmpDPGw2YfDL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JmpDPGw2YfDL",
        "outputId": "7266e459-6ba4-45c5-b9d9-446629b441d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter breakdown per module:\n",
            "- attn: 2362368 parameters\n",
            "- linear1: 2362368 parameters\n",
            "- linear2: 2360064 parameters\n",
            "- norm1: 1536 parameters\n",
            "- norm2: 1536 parameters\n",
            "\n",
            "Total parameters: 7087872\n"
          ]
        }
      ],
      "source": [
        "print(\"Parameter breakdown per module:\")\n",
        "for name, module in model.named_children():\n",
        "    num_params = sum(p.numel() for p in module.parameters())\n",
        "    print(f\"- {name}: {num_params} parameters\")\n",
        "\n",
        "total_params_manual = sum(sum(p.numel() for p in module.parameters()) for name, module in model.named_children())\n",
        "print(f\"\\nTotal parameters: {total_params_manual}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f15ff89",
      "metadata": {
        "id": "2f15ff89"
      },
      "source": [
        "Exercise 2: *GPT2* Parameters (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a7d6ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a7d6ee",
        "outputId": "e351d5bc-ebee-428b-970c-c45e1def1342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2Model(\n",
            "  (wte): Embedding(50257, 768)\n",
            "  (wpe): Embedding(1024, 768)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (h): ModuleList(\n",
            "    (0-11): 12 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2Attention(\n",
            "        (c_attn): Conv1D(nf=2304, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=768)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D(nf=3072, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=3072)\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "Total # of params: 124.44M\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Model\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "def count_params(model):\n",
        "    params: int = sum(p.numel() for p in model.parameters())\n",
        "    return f\"{params / 1e6:.2f}M\"\n",
        "\n",
        "print(model)\n",
        "print(\"Total # of params:\", count_params(model))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
