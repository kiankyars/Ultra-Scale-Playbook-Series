{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kiankyars/Ultra-Scale-Playbook-Series/blob/main/5_1d_parallelism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultra-Scale Playbook: Part 5 - Scaling Data Parallelism and Gradient Accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook covers key concepts about scaling LLM training through:\n",
    "- Understanding the relationship between global batch size, gradient accumulation, and data parallelism\n",
    "- Calculating optimal configurations for distributed training\n",
    "- Recognizing the limits of data parallelism\n",
    "- Practical examples of batch size calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fundamental Equation\n",
    "Global batch size (GBS) is determined by:\n",
    "\n",
    "$$ GBS = MBS \\times GA \\times DP $$\n",
    "\n",
    "Where:\n",
    "- MBS: Maximum Local Batch Size (largest batch that fits on a single GPU)\n",
    "- GA: Gradient Accumulation Steps\n",
    "- DP: Data Parallelism (number of GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradeoffs in Parallelization\n",
    "We can trade between gradient accumulation (GA) and data parallelism (DP):\n",
    "- More GA steps = sequential processing (slower but needs fewer GPUs)\n",
    "- More DP = parallel processing (faster but needs more GPUs)\n",
    "\n",
    "In practice, maximize DP first since it's inherently parallel, then use GA to reach target batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Example Setup\n",
    "1. Determine target global batch size in tokens (from literature/experiments)\n",
    "2. Find maximum local batch size (MBS) by increasing until GPU memory is full\n",
    "3. Determine available GPUs for DP\n",
    "4. Calculate required GA steps to reach target GBS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Example model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size=4096, hidden_size=2048):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_training_config(target_gbs, mbs, num_gpus):\n",
    "    \"\"\"\n",
    "    Calculate required gradient accumulation steps for target global batch size\n",
    "    \n",
    "    Args:\n",
    "        target_gbs: Desired global batch size\n",
    "        mbs: Maximum local batch size per GPU\n",
    "        num_gpus: Number of available GPUs\n",
    "        \n",
    "    Returns:\n",
    "        ga_steps: Required gradient accumulation steps\n",
    "        effective_gbs: Achieved global batch size\n",
    "    \"\"\"\n",
    "    ga_steps = target_gbs / (mbs * num_gpus)\n",
    "    \n",
    "    # GA steps must be at least 1\n",
    "    if ga_steps < 1:\n",
    "        print(f\"Warning: GA steps {ga_steps} < 1. You have more GPUs than needed.\")\n",
    "        print(\"Options:\")\n",
    "        print(\"1. Don't use all GPUs\")\n",
    "        print(\"2. Increase global batch size\")\n",
    "        print(\"3. Use smaller MBS for faster processing\")\n",
    "        ga_steps = 1\n",
    "    \n",
    "    effective_gbs = mbs * num_gpus * ga_steps\n",
    "    return ga_steps, effective_gbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate Training Configuration\n",
    "Given:\n",
    "- Target global batch size: 4 million tokens\n",
    "- Sequence length: 4,000 tokens\n",
    "- Maximum local batch size: 2 samples per GPU\n",
    "\n",
    "Calculate:\n",
    "1. How many GPUs are needed if we want GA steps = 1?\n",
    "2. If we have 128 GPUs, how many GA steps are needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "target_gbs_tokens = 4_000_000\n",
    "seq_length = 4000\n",
    "mbs = 2\n",
    "\n",
    "# Convert to samples (since MBS is in samples)\n",
    "target_gbs_samples = target_gbs_tokens / seq_length\n",
    "\n",
    "# Solution for question 1\n",
    "ga_steps = 1\n",
    "required_gpus = target_gbs_samples / (mbs * ga_steps)\n",
    "print(f\"Question 1: Need {required_gpus} GPUs for GA steps = 1\")\n",
    "\n",
    "# Solution for question 2\n",
    "num_gpus = 128\n",
    "ga_steps = target_gbs_samples / (mbs * num_gpus)\n",
    "print(f\"Question 2: Need {ga_steps} GA steps with 128 GPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Memory Estimation\n",
    "Estimate the memory requirements for a forward pass of our SimpleModel with:\n",
    "- Input size: 4096\n",
    "- Hidden size: 2048\n",
    "- Batch size: 4\n",
    "- Precision: float32 (4 bytes per parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def estimate_memory(input_size, hidden_size, batch_size):\n",
    "    # Calculate parameter memory\n",
    "    params = (input_size * hidden_size) + (hidden_size * hidden_size) + (hidden_size * input_size)\n",
    "    param_memory = params * 4  # 4 bytes per float32\n",
    "    \n",
    "    # Calculate activation memory (simplified estimation)\n",
    "    # We store input + 2 hidden layer outputs + final output\n",
    "    activation_memory = (input_size * batch_size + \n",
    "                        2 * hidden_size * batch_size + \n",
    "                        input_size * batch_size) * 4\n",
    "    \n",
    "    total_memory = (param_memory + activation_memory) / (1024 ** 2)  # Convert to MB\n",
    "    return total_memory\n",
    "\n",
    "memory_mb = estimate_memory(4096, 2048, 4)\n",
    "print(f\"Estimated memory requirement: {memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of Data Parallelism\n",
    "- Communication overhead grows with more GPUs\n",
    "- Ring latency becomes limiting factor (~512 GPUs with current tech)\n",
    "- Memory constraints for larger models (even single samples may not fit)\n",
    "\n",
    "When these limits are hit, we need to explore:\n",
    "1. Tensor Parallelism (splitting model layers across GPUs)\n",
    "2. Pipeline Parallelism (splitting model layers sequentially)\n",
    "3. Fully Sharded Data Parallelism (ZeRO, FSDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "1. What happens to throughput when you add GPUs beyond the network's capacity?\n",
    "   a) Increases linearly\n",
    "   b) Decreases due to communication overhead\n",
    "   c) Stays constant\n",
    "   \n",
    "2. If your GA steps calculation gives 0.5, what does this mean?\n",
    "   a) You need to double your batch size\n",
    "   b) You have more GPUs than needed for your target batch size\n",
    "   c) Your model is too large for the GPUs\n",
    "   \n",
    "3. What's the main advantage of data parallelism over gradient accumulation?\n",
    "   a) It's inherently parallel\n",
    "   b) It uses less memory\n",
    "   c) It provides better model accuracy\n",
    "   \n",
    "4. What's the first thing to determine when setting up a distributed training run?\n",
    "   a) Number of available GPUs\n",
    "   b) Target global batch size\n",
    "   c) Learning rate\n",
    "\n",
    "Answers:\n",
    "1. b) Decreases due to communication overhead\n",
    "2. b) You have more GPUs than needed for your target batch size\n",
    "3. a) It's inherently parallel\n",
    "4. b) Target global batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Data parallelism is the first dimension (1D) of parallelization\n",
    "- Combine DP with gradient accumulation to reach target batch sizes\n",
    "- There are practical limits to how much DP can help\n",
    "- Next we'll explore other parallelism techniques (tensor, pipeline, and FSDP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
