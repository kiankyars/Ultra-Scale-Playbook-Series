{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiankyars/Ultra-Scale-Playbook-Series/blob/main/notebooks/bonus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYnvpp5IpEZ8",
        "outputId": "0ca2edfa-577b-4f95-9a9f-ecfdc4672550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# Google Colab Exercise Notebook: Transformer Training with Gradient Accumulation\n",
        "\n",
        "# This notebook explores training a GPT-2 model on a single GPU, with an exercise on gradient accumulation.\n",
        "\n",
        "!pip install torch transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Config, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkB2wK_ak-BK"
      },
      "outputs": [],
      "source": [
        "# Exercise:\n",
        "# Complete the train_with_accumulation function by:\n",
        "# 1. Calculating micro_steps\n",
        "# 2. Implementing the forward/backward pass with loss accumulation\n",
        "# 3. Adding the optimizer step after all micro-batches\n",
        "\n",
        "# Define a standard GPT-2 model\n",
        "config = GPT2Config(\n",
        "    n_embd=768,    # Standard hidden dimension\n",
        "    n_layer=12,    # 12 layers\n",
        "    n_head=12,     # 12 attention heads\n",
        "    vocab_size=50257,  # Full vocab size\n",
        "    n_positions=1024   # Standard sequence length\n",
        ")\n",
        "model = GPT2Model(config).cuda()\n",
        "\n",
        "# Training step function\n",
        "def train_step(model, optimizer, input_ids):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids)\n",
        "    loss = torch.mean(outputs.last_hidden_state)  # Dummy loss for simplicity\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Batch size experiment\n",
        "def run_training(model, batch_size, seq_len=1024, steps=5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    for step in range(steps):\n",
        "        input_ids = torch.randint(0, 50257, (batch_size, seq_len)).cuda()\n",
        "        loss = train_step(model, optimizer, input_ids)\n",
        "        print(f\"Step {step}, Batch Size {batch_size}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Gradient accumulation exercise\n",
        "def train_with_accumulation(model, total_batch_size=8, micro_batch_size=2, seq_len=1024, steps=5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    micro_steps = None  # TODO: Calculate number of micro-batches\n",
        "\n",
        "    for step in range(steps):\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = 0\n",
        "\n",
        "        for _ in range(micro_steps):\n",
        "            input_ids = torch.randint(0, 50257, (micro_batch_size, seq_len)).cuda()\n",
        "            # TODO: Implement forward pass, backward pass, and loss accumulation\n",
        "            # Do not step the optimizer until all micro-batches are processed\n",
        "\n",
        "        # TODO: Perform the optimizer step after accumulating gradients\n",
        "        print(f\"Step {step}, Average Loss: {total_loss/micro_steps:.4f}\")\n",
        "\n",
        "    print(\"Finished training with gradient accumulation\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting GPT-2 training experiments...\")\n",
        "\n",
        "    # Run with different batch sizes\n",
        "    print(\"\\nBatch Size Effects:\")\n",
        "    for i in range(1, 11, 1):\n",
        "        run_training(model, batch_size=i)\n",
        "\n",
        "    # Gradient accumulation exercise\n",
        "    print(\"\\nGradient Accumulation Exercise:\")\n",
        "    train_with_accumulation(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsWKPn5Ko8Qt",
        "outputId": "3c54e5c4-2251-46c8-82a5-4bef3d34356e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting GPT-2 training experiments...\n",
            "\n",
            "Gradient Accumulation Exercise:\n",
            "Step 0, Average Loss: 10.9998\n",
            "Step 1, Average Loss: 10.9842\n",
            "Step 2, Average Loss: 10.9824\n",
            "Step 3, Average Loss: 11.0622\n",
            "Step 4, Average Loss: 11.0066\n",
            "Step 5, Average Loss: 11.0169\n",
            "Step 6, Average Loss: 11.0006\n",
            "Step 7, Average Loss: 10.9932\n",
            "Step 8, Average Loss: 11.0006\n",
            "Step 9, Average Loss: 11.0247\n",
            "Step 10, Average Loss: 11.0560\n",
            "Step 11, Average Loss: 11.0714\n",
            "Step 12, Average Loss: 11.0651\n",
            "Step 13, Average Loss: 11.0603\n",
            "Step 14, Average Loss: 11.0382\n",
            "Step 15, Average Loss: 11.0371\n",
            "Step 16, Average Loss: 11.0261\n",
            "Step 17, Average Loss: 11.0205\n",
            "Step 18, Average Loss: 11.0276\n",
            "Step 19, Average Loss: 11.0250\n",
            "Finished training with gradient accumulation\n"
          ]
        }
      ],
      "source": [
        "# Exercise:\n",
        "# Complete the train_with_accumulation function by:\n",
        "# 1. Calculating micro_steps\n",
        "# 2. Implementing the forward/backward pass with loss accumulation\n",
        "# 3. Adding the optimizer step after all micro-batches\n",
        "\n",
        "# Define a standard GPT-2 model\n",
        "config = GPT2Config(\n",
        "    n_embd=768,    # Standard hidden dimension\n",
        "    n_layer=12,    # 12 layers\n",
        "    n_head=12,     # 12 attention heads\n",
        "    vocab_size=50257,  # Full vocab size\n",
        "    n_positions=1024   # Standard sequence length\n",
        ")\n",
        "model = GPT2LMHeadModel(config).cuda()\n",
        "\n",
        "# Training step function\n",
        "def train_step(model, optimizer, input_ids):\n",
        "    optimizer.zero_grad()\n",
        "    # Shift inputs and targets for next-token prediction\n",
        "    input_ids, labels = input_ids[:, :-1], input_ids[:, 1:]\n",
        "    outputs = model(input_ids, labels=labels)\n",
        "    loss = outputs.loss  # Cross-entropy loss from model\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Gradient accumulation implementation\n",
        "def train_with_accumulation(model, total_batch_size=8, micro_batch_size=2, seq_len=1024, steps=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    micro_steps = total_batch_size // micro_batch_size\n",
        "\n",
        "    for step in range(steps):\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = 0\n",
        "\n",
        "        for _ in range(micro_steps):\n",
        "            input_ids = torch.randint(0, 50257, (micro_batch_size, seq_len)).cuda()\n",
        "            input_ids, labels = input_ids[:, :-1], input_ids[:, 1:]\n",
        "            outputs = model(input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()  # Gradients accumulate\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        optimizer.step()  # Update after all micro-batches\n",
        "        print(f\"Step {step}, Average Loss: {total_loss/micro_steps:.4f}\")\n",
        "\n",
        "    print(\"Finished training with gradient accumulation\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting GPT-2 training experiments...\")\n",
        "\n",
        "    # Gradient accumulation exercise\n",
        "    print(\"\\nGradient Accumulation Exercise:\")\n",
        "    train_with_accumulation(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtkgbpMSp4yt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPbB8e3145AA9AuEblmjney",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
