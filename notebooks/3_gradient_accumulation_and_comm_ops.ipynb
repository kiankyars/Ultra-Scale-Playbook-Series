{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e573b2",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kiankyars/Ultra-Scale-Playbook-Series/blob/main/notebooks/3_gradient_accumulation_and_comm_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73039ce3",
   "metadata": {},
   "source": [
    "# Ultra-Scale Playbook: Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81675cd0",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, you'll learn about:\n",
    "- What gradient accumulation is and why it helps with memory efficiency\n",
    "- The relationship between microbatch size and global batch size\n",
    "- How activation memory is reduced using recomputation and accumulation\n",
    "- Collective communication primitives for parallel processing on multiple GPUs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bd2ab",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Accumulation\n",
    "\n",
    "Gradient accumulation is a technique to simulate larger batch sizes than GPU memory would normally allow by:\n",
    "- Performing multiple forward and backward passes on smaller \"microbatches\"\n",
    "- Accumulating gradients instead of updating weights immediately\n",
    "- Performing the optimizer step only after accumulating over `N` microbatches\n",
    "\n",
    "### Equation\n",
    "If `microbatch_size = m` and `gradient_accumulation_steps = n`, then:\n",
    "\n",
    "```python\n",
    "global_batch_size = microbatch_size * gradient_accumulation_steps\n",
    "```\n",
    "\n",
    "By averaging the gradients before applying the optimizer, training remains consistent regardless of `n`.\n",
    "\n",
    "### Visual Explanation\n",
    "\n",
    "1. Forward+Backward (MB1): Accumulate Gradients\n",
    "2. Forward+Backward (MB2): Accumulate Gradients\n",
    "3. Forward+Backward (MB3): Accumulate Gradients\n",
    "4. Optimizer Step: Apply average of accumulated gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e972f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "model = nn.Linear(10, 1).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "gradient_accumulation_steps = 4\n",
    "microbatch_size = 2\n",
    "\n",
    "# Fake data\n",
    "inputs = torch.randn(gradient_accumulation_steps * microbatch_size, 10).cuda()\n",
    "targets = torch.randn(gradient_accumulation_steps * microbatch_size, 1).cuda()\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "for i in range(gradient_accumulation_steps):\n",
    "    start = i * microbatch_size\n",
    "    end = (i + 1) * microbatch_size\n",
    "    x = inputs[start:end]\n",
    "    y = targets[start:end]\n",
    "\n",
    "    output = model(x)\n",
    "    loss = loss_fn(output, y)\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"Step {i+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "optimizer.step()\n",
    "print(\"Optimizer step performed with accumulated gradients.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab4ee5",
   "metadata": {},
   "source": [
    "\n",
    "## üß† Exercise 1: Customize Accumulation\n",
    "\n",
    "Modify the code to:\n",
    "1. Use Adam optimizer\n",
    "2. Increase `gradient_accumulation_steps` to 8\n",
    "3. Print total accumulated gradient norm before the optimizer step\n",
    "\n",
    "> ‚úÖ **Hint**: Use `torch.nn.utils.clip_grad_norm_` to get the norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc027ac",
   "metadata": {},
   "source": [
    "\n",
    "## Combining with Activation Recomputation\n",
    "\n",
    "Activation recomputation (a.k.a. gradient checkpointing) is compatible with gradient accumulation.\n",
    "\n",
    "Using both allows:\n",
    "- Lower memory usage from forward activations\n",
    "- Larger effective batch sizes\n",
    "\n",
    "> This memory-compute trade-off is critical for training very large LLMs on limited hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b772e",
   "metadata": {},
   "source": [
    "\n",
    "## Communication Primitives: AllReduce and More\n",
    "\n",
    "When training across multiple GPUs or nodes, we use **collective operations** such as:\n",
    "\n",
    "- `broadcast`: Send data from one GPU to all others\n",
    "- `reduce`: Combine tensors from all GPUs to one (e.g. sum)\n",
    "- `all_reduce`: Like reduce, but all GPUs get the result\n",
    "- `gather` / `scatter`: Move data from/to a root GPU\n",
    "- `barrier`: Synchronize all GPUs at a point\n",
    "\n",
    "These are provided by `torch.distributed` and are essential in **Data Parallelism**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f887d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a broadcast example - requires torch.distributed context to actually run\n",
    "# Uncomment and configure if running in a distributed setup\n",
    "\n",
    "# import torch.distributed as dist\n",
    "# dist.init_process_group(\"nccl\", rank=..., world_size=...)\n",
    "# if dist.get_rank() == 0:\n",
    "#     tensor = torch.arange(5).cuda()\n",
    "# else:\n",
    "#     tensor = torch.zeros(5).cuda()\n",
    "# dist.broadcast(tensor, src=0)\n",
    "# print(f\"Rank {dist.get_rank()}: {tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26491111",
   "metadata": {},
   "source": [
    "\n",
    "## üß† Exercise 2: Simulate Reduce\n",
    "\n",
    "Write code to:\n",
    "- Create random tensors on multiple GPUs (simulate via `.to()` if needed)\n",
    "- Use `torch.stack()` and `.sum(dim=0)` to simulate a manual reduce operation\n",
    "- Compare with actual `all_reduce` result (if using multi-GPU environment)\n",
    "\n",
    "> üö® If you only have 1 GPU, simulate across different tensors on CPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54fb30",
   "metadata": {},
   "source": [
    "\n",
    "## ‚ùì Quiz\n",
    "\n",
    "1. **What is the main benefit of gradient accumulation?**\n",
    "   - A) Faster training\n",
    "   - B) Lower memory usage per step ‚úÖ\n",
    "   - C) Higher accuracy\n",
    "   - D) Requires fewer GPUs\n",
    "\n",
    "2. **What does `global_batch_size` equal?**\n",
    "   - A) Number of GPUs √ó microbatch size\n",
    "   - B) `gradient_accumulation_steps` √ó `microbatch_size` ‚úÖ\n",
    "\n",
    "3. **Which operations synchronize all nodes before continuing?**\n",
    "   - A) scatter\n",
    "   - B) barrier ‚úÖ\n",
    "   - C) gather\n",
    "   - D) broadcast\n",
    "\n",
    "4. **What is the difference between reduce and all_reduce?**\n",
    "   - Reduce sends result to one GPU, all_reduce sends result to all ‚úÖ\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
