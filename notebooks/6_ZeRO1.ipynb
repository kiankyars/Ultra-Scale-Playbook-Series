{
    "cells": [
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "<a href=\"https://colab.research.google.com/github/kiankyars/Ultra-Scale-Playbook-Series/blob/main/6_ZeRO1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
        ]
        },
        {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Ultra-Scale Playbook: Part 6 - Zero Redundancy Optimizer (ZeRO) Introduction"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Overview\n",
       "This notebook covers:\n",
       "- Memory redundancies in naive data parallelism\n",
       "- ZeRO optimization stages (01-03)\n",
       "- How ZeRO-1 partitions optimizer states\n",
       "- Communication patterns in ZeRO\n",
       "- Practical implementations and tradeoffs"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Key Concepts"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Memory Redundancy in Data Parallelism\n",
       "In naive DP, each GPU stores:\n",
       "- Full copy of model parameters\n",
       "- Full copy of gradients\n",
       "- Full copy of optimizer states\n",
       "\n",
       "This creates significant memory redundancy that ZeRO aims to eliminate."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### ZeRO Optimization Stages\n",
       "ZeRO progressively eliminates redundancies:\n",
       "1. **ZeRO-1**: Partitions optimizer states\n",
       "2. **ZeRO-2**: Partitions gradients + optimizer states\n",
       "3. **ZeRO-3**: Partitions parameters + gradients + optimizer states\n",
       "\n",
       "Today we focus on ZeRO-1."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### ZeRO-1: Optimizer State Partitioning\n",
       "- Optimizer states split across GPUs\n",
       "- Each GPU only updates its portion of parameters\n",
       "- Requires new communication patterns:\n",
       "  - Replace all-reduce with reduce-scatter\n",
       "  - Add all-gather after optimizer step"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Memory Calculations"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def calculate_memory_usage(num_params, use_fp32_grad_accum=False):\n",
       "    \"\"\"\n",
       "    Calculate memory usage for different components\n",
       "    \n",
       "    Args:\n",
       "        num_params: Number of model parameters\n",
       "        use_fp32_grad_accum: Whether to accumulate gradients in fp32\n",
       "        \n",
       "    Returns:\n",
       "        Dictionary of memory usage in bytes\n",
       "    \"\"\"\n",
       "    memory = {}\n",
       "    \n",
       "    # Model parameters in bf16/fp16\n",
       "    memory['params_bf16'] = num_params * 2\n",
       "    \n",
       "    # Gradients in bf16/fp16\n",
       "    memory['gradients_bf16'] = num_params * 2\n",
       "    \n",
       "    # Parameters in fp32 (for optimizer)\n",
       "    memory['params_fp32'] = num_params * 4\n",
       "    \n",
       "    # Optimizer states (momentum + variance)\n",
       "    memory['optim_states'] = num_params * 8\n",
       "    \n",
       "    # Gradient accumulation in fp32 (optional)\n",
       "    memory['grad_accum_fp32'] = num_params * 4 if use_fp32_grad_accum else 0\n",
       "    \n",
       "    # Total memory\n",
       "    memory['total'] = (memory['params_bf16'] + \n",
       "                      memory['gradients_bf16'] + \n",
       "                      memory['params_fp32'] + \n",
       "                      memory['optim_states'] + \n",
       "                      memory['grad_accum_fp32'])\n",
       "    \n",
       "    return memory"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ZeRO-1 Implementation Example"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "import torch.distributed as dist\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "# Simplified ZeRO-1 style optimizer\n",
       "class PartitionedOptimizer:\n",
       "    def __init__(self, params, optimizer_class, num_partitions):\n",
       "        self.num_partitions = num_partitions\n",
       "        self.rank = dist.get_rank() if dist.is_initialized() else 0\n",
       "        \n",
       "        # Partition parameters\n",
       "        param_groups = self._partition_parameters(list(params))\n",
       "        \n",
       "        # Create optimizer only for this partition\n",
       "        self.optimizer = optimizer_class(param_groups[self.rank])\n",
       "    \n",
       "    def _partition_parameters(self, params):\n",
       "        \"\"\"Split parameters into equal partitions\"\"\"\n",
       "        partition_size = len(params) // self.num_partitions\n",
       "        return [params[i*partition_size:(i+1)*partition_size] \n",
       "                for i in range(self.num_partitions)]\n",
       "    \n",
       "    def step(self):\n",
       "        \"\"\"Perform optimizer step only on this partition\"\"\"\n",
       "        self.optimizer.step()\n",
       "        \n",
       "        # In real implementation, would need all-gather here\n",
       "        # to share updated parameters across all ranks\n",
       "        \n",
       "    def zero_grad(self):\n",
       "        self.optimizer.zero_grad()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Interactive Exercises"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Exercise 1: Memory Savings Calculation\n",
       "Calculate the memory savings from ZeRO-1 for:\n",
       "- Model with 7B parameters\n",
       "- 8 GPUs\n",
       "- Without fp32 gradient accumulation\n",
       "\n",
       "Compare naive DP vs ZeRO-1 memory usage"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Your solution here\n",
       "num_params = 7_000_000_000\n",
       "num_gpus = 8\n",
       "\n",
       "# Naive DP memory (per GPU)\n",
       "naive_mem = calculate_memory_usage(num_params)['total']\n",
       "\n",
       "# ZeRO-1 memory (per GPU)\n",
       "zero_mem = calculate_memory_usage(num_params)['total']\n",
       "# Only optimizer states are partitioned\n",
       "zero_mem -= (calculate_memory_usage(num_params)['optim_states'] * (1 - 1/num_gpus))\n",
       "\n",
       "print(f\"Naive DP memory per GPU: {naive_mem / (1024**3):.2f} GB\")\n",
       "print(f\"ZeRO-1 memory per GPU: {zero_mem / (1024**3):.2f} GB\")\n",
       "print(f\"Memory savings: {(naive_mem - zero_mem) / (1024**3):.2f} GB per GPU\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Exercise 2: Communication Pattern Analysis\n",
       "Compare the communication volume for:\n",
       "1. Naive DP (all-reduce)\n",
       "2. ZeRO-1 (reduce-scatter + all-gather)\n",
       "\n",
       "Assume:\n",
       "- 1B parameters\n",
       "- bf16 precision (2 bytes per parameter)\n",
       "- 8 GPUs"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Your solution here\n",
       "num_params = 1_000_000_000\n",
       "num_gpus = 8\n",
       "bytes_per_param = 2\n",
       "\n",
       "# Naive DP: all-reduce (2x parameter size)\n",
       "naive_comm = 2 * num_params * bytes_per_param\n",
       "\n",
       "# ZeRO-1: reduce-scatter (1x) + all-gather (1x)\n",
       "zero_comm = num_params * bytes_per_param * 2\n",
       "\n",
       "print(f\"Naive DP communication: {naive_comm / (1024**3):.2f} GB\")\n",
       "print(f\"ZeRO-1 communication: {zero_comm / (1024**3):.2f} GB\")\n",
       "print(f\"Communication difference: {(naive_comm - zero_comm) / (1024**3):.2f} GB\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ZeRO Communication Patterns\n",
       "\n",
       "### Training Step Comparison\n",
       "| Operation       | Vanilla DP         | ZeRO-1             |\n",
       "|-----------------|--------------------|--------------------|\n",
       "| Forward Pass    | Same               | Same               |\n",
       "| Backward Pass   | Same               | Same               |\n",
       "| Gradient Sync   | all-reduce         | reduce-scatter     |\n",
       "| Optimizer Step  | Full update        | Partitioned update |\n",
       "| Param Sync      | None               | all-gather         |"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Quiz\n",
       "1. What does ZeRO-1 partition across GPUs?\n",
       "   a) Parameters\n",
       "   b) Gradients\n",
       "   c) Optimizer states\n",
       "   \n",
       "2. What new communication operation does ZeRO-1 introduce?\n",
       "   a) broadcast\n",
       "   b) all-gather\n",
       "   c) reduce\n",
       "   \n",
       "3. Why can't we partition activations in data parallelism?\n",
       "   a) Each GPU processes different data\n",
       "   b) Activations are too small\n",
       "   c) It would hurt model accuracy\n",
       "   \n",
       "4. What's the main tradeoff in using ZeRO?\n",
       "   a) Memory savings vs communication overhead\n",
       "   b) Speed vs accuracy\n",
       "   c) Model size vs batch size\n",
       "\n",
       "Answers:\n",
       "1. c) Optimizer states\n",
       "2. b) all-gather\n",
       "3. a) Each GPU processes different data\n",
       "4. a) Memory savings vs communication overhead"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Summary\n",
       "- ZeRO reduces memory redundancy in distributed training\n",
       "- ZeRO-1 partitions optimizer states across GPUs\n",
       "- Introduces new communication patterns (reduce-scatter + all-gather)\n",
       "- Provides significant memory savings with some communication overhead\n",
       "- Next we'll explore ZeRO-2 (gradient partitioning) and ZeRO-3 (parameter partitioning)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }