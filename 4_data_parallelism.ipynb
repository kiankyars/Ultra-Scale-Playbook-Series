{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c179993c",
      "metadata": {},
      "source": [
        "# Ultra-Scale Playbook: Part 4\n",
        "\n",
        "Welcome back! In this episode we dive into **Data Parallelism** and three key optimizations to maximize GPU utilization and minimize idle time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d68d99",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "- Recap of Data Parallelism (DP)\n",
        "- **Optimization 1:** Overlap gradient synchronization with backward pass\n",
        "- **Optimization 2:** Bucketed gradient reductions\n",
        "- **Optimization 3:** Combining DP with gradient accumulation via `no_sync()`\n",
        "- Hands-on exercises & quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4614498b",
      "metadata": {},
      "source": [
        "## 1 Data Parallelism Recap\n",
        "\n",
        "- We **replicate** the full model on each GPU (called *replicas*).\n",
        "- Each GPU processes a different **microbatch** in parallel: forward → backward.\n",
        "- To keep replicas in sync, we **all-reduce** their gradients **before** `optimizer.step()`.\n",
        "\n",
        "```python\n",
        "# pseudo-DDP loop\n",
        "for x_mb, y_mb in microbatches:\n",
        "    output = model(x_mb)        # each GPU\n",
        "    loss = loss_fn(output,y_mb)\n",
        "    loss.backward()             # triggers all-reduce of grads\n",
        "optimizer.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d142df8",
      "metadata": {},
      "source": [
        "### The naive inefficiency\n",
        "\n",
        "By default, DDP performs an all-reduce **after every** `backward()`.  If you have *N* microbatches per update, that is *N* communications—wasting GPU cycles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdf8c955",
      "metadata": {},
      "source": [
        "## 2 Optimization 1: Overlap gradient sync with backward pass\n",
        "\n",
        "**Idea:** As soon as each layer’s backward finishes, launch its all-reduce **while** computing the next layer’s backward.\n",
        "\n",
        "- Attaching a per-parameter hook lets DDP overlap communication & computation.\n",
        "\n",
        "```python\n",
        "def attach_sync_hook(model):\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad:\n",
        "            p.register_hook(lambda grad: dist.all_reduce(grad, op=dist.ReduceOp.SUM))\n",
        "```\n",
        "\n",
        "This reduces idle GPU time and can boost throughput by ~1.3×."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5a5f34",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "# Example: attach a simple all-reduce hook to each parameter\n",
        "def attach_sync_hook(model):\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad:\n",
        "            p.register_hook(lambda grad: dist.all_reduce(grad, op=dist.ReduceOp.SUM))\n",
        "\n",
        "# Usage in a DDP setup (pseudo)\n",
        "# dist.init_process_group('nccl', ...)\n",
        "# model = torch.nn.parallel.DistributedDataParallel(model)\n",
        "# attach_sync_hook(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e70566",
      "metadata": {},
      "source": [
        "## 3 Optimization 2: Bucketed gradient reductions\n",
        "\n",
        "**Idea:** Group many small gradient tensors into **buckets**, then do **one** all-reduce per bucket instead of per-tensor.\n",
        "\n",
        "- Large tensors amortize communication startup cost.\n",
        "- Buckets often map to layers or parameter groups.\n",
        "\n",
        "```python\n",
        "# pseudo-bucket logic\n",
        "bucket_size = 1_000_000  # number of elements\n",
        "buckets = []\n",
        "current = []\n",
        "for p in model.parameters():\n",
        "    current.append(p.grad.view(-1))\n",
        "    if sum(x.numel() for x in current) >= bucket_size:\n",
        "        buckets.append(torch.cat(current))\n",
        "        current.clear()\n",
        "# all_reduce each bucket once\n",
        "for b in buckets:\n",
        "    dist.all_reduce(b, op=dist.ReduceOp.SUM)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89369241",
      "metadata": {},
      "source": [
        "## 4 Optimization 3: DP + Gradient Accumulation → `no_sync()`\n",
        "\n",
        "When you **accumulate** gradients over *K* microbatches before calling `optimizer.step()`, you only need **one** all-reduce at the end.\n",
        "\n",
        "```python\n",
        "model = torch.nn.parallel.DistributedDataParallel(model)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "accum_steps = 4\n",
        "for i in range(accum_steps):\n",
        "    out = model(x[i])\n",
        "    loss = loss_fn(out, y[i])\n",
        "    if i < accum_steps - 1:\n",
        "        with model.no_sync():\n",
        "            loss.backward()  # no communication\n",
        "    else:\n",
        "        loss.backward()  # single all-reduce here\n",
        "optimizer.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c4d942",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# Pseudo-initialization\n",
        "# dist.init_process_group('nccl', rank=..., world_size=...)\n",
        "model = nn.Linear(128, 10).cuda()\n",
        "model = DDP(model)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Fake microbatches\n",
        "microbatches = [(torch.randn(8,128).cuda(), torch.randint(0,10,(8,)).cuda()) for _ in range(4)]\n",
        "optimizer.zero_grad()\n",
        "for i, (x_mb, y_mb) in enumerate(microbatches):\n",
        "    logits = model(x_mb)\n",
        "    loss = loss_fn(logits, y_mb)\n",
        "    if i < len(microbatches)-1:\n",
        "        with model.no_sync():\n",
        "            loss.backward()\n",
        "    else:\n",
        "        loss.backward()\n",
        "optimizer.step()\n",
        "print(\"Done with bucketed accumulation + single sync\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e7cd88c",
      "metadata": {},
      "source": [
        "## 🧠 Exercise 1: Implement Overlap Hook\n",
        "\n",
        "1. Copy the `attach_sync_hook` function above.\n",
        "2. Initialize a small DDP model (e.g. `nn.Linear`) in a single-node 2-GPU setup.\n",
        "3. Measure throughput **with** and **without** the hook to see the speed-up."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d280f04",
      "metadata": {},
      "source": [
        "## 🧠 Exercise 2: Bucket via Parameter Groups\n",
        "\n",
        "1. Partition model parameters into 2 buckets (e.g. first half, second half).\n",
        "2. After a backward pass, manually gather all `.grad` from each bucket, concat, and all-reduce once.\n",
        "3. Compare to default DDP behavior by timing a few iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05951d4c",
      "metadata": {},
      "source": [
        "## ❓ Quiz\n",
        "\n",
        "1. **Why overlap gradient sync with backward?**  \n",
        "   A) To reduce peak memory usage  \n",
        "   B) To hide communication under computation ✅  \n",
        "   C) To increase batch size  \n",
        "   D) To simplify code\n",
        "\n",
        "2. **Bucketed reductions** help by:  \n",
        "   A) Reducing number of all-reduce calls ✅  \n",
        "   B) Eliminating gradient accumulation  \n",
        "   C) Avoiding model replication  \n",
        "   D) Merging forward & backward\n",
        "\n",
        "3. **`model.no_sync()`** is used to:  \n",
        "   A) Disable gradient computation  \n",
        "   B) Temporarily disable DDP’s all-reduce hooks ✅  \n",
        "   C) Stop optimizer steps  \n",
        "   D) Synchronize parameters\n",
        "\n",
        "4. **After `loss.backward()` inside `no_sync()`, when do gradients sync?**  \n",
        "   A) Immediately, per parameter  \n",
        "   B) At the next `loss.backward()` call ✅  \n",
        "   C) At optimizer initialization  \n",
        "   D) They never sync"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
