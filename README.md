# Ultra-Scale-Playbook-Series
Learn how to scale LLM training efficiently on GPUs and clusters.

This is a companion repo to the YouTube video series that walks through the Hugging Face [Ultra-Scale Playbook](https://huggingface.co/docs/transformers/main/en/performance), with interactive notebooks per episode.

---

## Target Audience
- Intermediate ML/LLM developers
- Comfortable with PyTorch and basic Transformer training
- New to GPU cluster-level distributed training

---

## ðŸ“º Episodes & Notebooks

| Episode | Title | Notebook |
|--------:|-----------------------------|-----------------------------|
| 01 | What is Scaling? | [`01_scaling_basics.ipynb`](01_scaling_basics.ipynb) |
| 02 | Training on One GPU | *Coming Soon* |
| 03 | Memory in Transformers | *Coming Soon* |
| 04 | Activation Recomputation | *Coming Soon* |
| 05 | Gradient Accumulation | *Coming Soon* |
| 06 | Data Parallelism | *Coming Soon* |
| 07 | Optimizing DDP | *Coming Soon* |
| 08 | ZeRO Strategies | *Coming Soon* |
| 09 | Tensor Parallelism | *Coming Soon* |
| 10 | TP in Transformers | *Coming Soon* |
| 11 | Sequence Parallelism | *Coming Soon* |
| 12 | Context Parallelism | *Coming Soon* |
| 13 | Pipeline Parallelism | *Coming Soon* |
| 14 | Full 5D Strategy | *Coming Soon* |
| 15 | Real Benchmarks | *Coming Soon* |
| 16 | FlashAttention & Kernels | *Coming Soon* |

---

## Setup Instructions

```bash
git clone https://github.com/neuralkian/ultra-scale-playbook-series.git
cd ultra-scale-playbook-series
```

- Use colab or local development to open notebooks!
